[ai]
    # AI provider to use: 'groq' (FASTEST, FREE), 'openai', 'gemini', or 'lmstudio' (local)
    provider = "lmstudio"

[lmstudio]
    # LM Studio Local API Configuration
    # Make sure LM Studio is running and the server is started before using this provider
    
    # API URL (default: http://localhost:1234/v1/chat/completions)
    # You can change the port if you configured LM Studio differently
    apiUrl = "http://127.0.0.1:1234/v1/chat/completions"
    
    # Model name as shown in LM Studio
    # Leave empty to use the currently loaded model in LM Studio
    # Examples: "llama-3.2-3b-instruct", "mistral-7b-instruct", "phi-3-mini"
    model = "qwen3-8b"
    
    # API key (optional, usually not required for local usage)
    # Some LM Studio configurations may require this
    apiKey = ""

[behavior]
    # Ticks between action checks (20 ticks = 1 second)
    actionTickDelay = 20
    
    # Allow Steves to respond in chat
    enableChatResponses = true
    
    # Maximum number of Steves that can be active simultaneously
    maxActiveSteves = 10

